{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#importing the neccessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "import random\n",
    "from keras import optimizers, losses, activations, models\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
    "from keras.layers import Dense, Input, Dropout, Convolution1D, MaxPool1D, GlobalMaxPool1D, GlobalAveragePooling1D, \\\n",
    "    concatenate\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading preprocessed training and test datasets using pandas\n",
    "df_train = pd.read_csv(\"mitbih_train.csv\", header=None)\n",
    "df_train = df_train.sample(frac=1)\n",
    "df_test = pd.read_csv(\"mitbih_test.csv\", header=None)\n",
    "\n",
    "#get train & test data in desired datatype\n",
    "Y = np.array(df_train[187].values).astype(np.int8)\n",
    "X = np.array(df_train[list(range(187))].values)[..., np.newaxis]\n",
    "Y_test = np.array(df_test[187].values).astype(np.int8)\n",
    "X_test = np.array(df_test[list(range(187))].values)[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model function\n",
    "def get_model():\n",
    "    #number of categories in our problem\n",
    "    nclass = 5\n",
    "    #shape of input\n",
    "    inp = Input(shape=(187, 1))\n",
    "    #activations allows models to take into account non-linear relationships. relu = rectified linear activation\n",
    "    #kernel is the matrix of weights that is convolved on the input. Often odd numbers (5 and 3 in this case)\n",
    "    #pooling is to downsample the input\n",
    "    #1 -- 16 filters\n",
    "    img_1 = Convolution1D(16, kernel_size=5, activation=activations.relu, padding=\"valid\")(inp)\n",
    "    img_1 = Convolution1D(16, kernel_size=5, activation=activations.relu, padding=\"valid\")(img_1)\n",
    "    img_1 = MaxPool1D(pool_size=2)(img_1)\n",
    "    img_1 = Dropout(rate=0.1)(img_1) #to prevent overfitting\n",
    "    #2 -- 16x2 filters\n",
    "    img_1 = Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
    "    img_1 = Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
    "    img_1 = MaxPool1D(pool_size=2)(img_1)\n",
    "    img_1 = Dropout(rate=0.1)(img_1) #to prevent overfitting\n",
    "    #3 -- 16x2 filters\n",
    "    img_1 = Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
    "    img_1 = Convolution1D(32, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
    "    img_1 = MaxPool1D(pool_size=2)(img_1)\n",
    "    img_1 = Dropout(rate=0.1)(img_1) #to prevent overfitting\n",
    "    #4 -- 16x16 filters\n",
    "    img_1 = Convolution1D(256, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
    "    img_1 = Convolution1D(256, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
    "    img_1 = GlobalMaxPool1D()(img_1)\n",
    "    img_1 = Dropout(rate=0.2)(img_1) #to prevent overfitting\n",
    "    \n",
    "    #Dense is the layer type. \n",
    "    #Dense is a standard layer type that works for most cases. \n",
    "    #In a dense layer, all nodes in the previous layer connect to the nodes in the current layer.\n",
    "    dense_1 = Dense(64, activation=activations.relu, name=\"dense_1\")(img_1)\n",
    "    dense_1 = Dense(64, activation=activations.relu, name=\"dense_2\")(dense_1)\n",
    "    dense_1 = Dense(nclass, activation=activations.softmax, name=\"dense_3_mitbih\")(dense_1)\n",
    "\n",
    "    model = models.Model(inputs=inp, outputs=dense_1)\n",
    "    opt = optimizers.Adam(0.001)\n",
    "\n",
    "    model.compile(optimizer=opt, loss=losses.sparse_categorical_crossentropy, metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 187, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 183, 16)           96        \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 179, 16)           1296      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 89, 16)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 89, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 87, 32)            1568      \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 85, 32)            3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 42, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 42, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 40, 32)            3104      \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 38, 32)            3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 19, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 19, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 17, 256)           24832     \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 15, 256)           196864    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_3_mitbih (Dense)       (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 254,901\n",
      "Trainable params: 254,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 78798 samples, validate on 8756 samples\n",
      "Epoch 1/1000\n",
      " - 126s - loss: 0.3493 - acc: 0.9006 - val_loss: 0.1749 - val_acc: 0.9497\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.94975, saving model to baseline_cnn_mitbih.h5\n",
      "Epoch 2/1000\n",
      " - 112s - loss: 0.1815 - acc: 0.9499 - val_loss: 0.1449 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.94975 to 0.95831, saving model to baseline_cnn_mitbih.h5\n",
      "Epoch 3/1000\n",
      " - 108s - loss: 0.1443 - acc: 0.9611 - val_loss: 0.1003 - val_acc: 0.9732\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.95831 to 0.97316, saving model to baseline_cnn_mitbih.h5\n",
      "Epoch 4/1000\n",
      " - 112s - loss: 0.1247 - acc: 0.9660 - val_loss: 0.0895 - val_acc: 0.9748\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.97316 to 0.97476, saving model to baseline_cnn_mitbih.h5\n",
      "Epoch 5/1000\n",
      " - 113s - loss: 0.1127 - acc: 0.9692 - val_loss: 0.0883 - val_acc: 0.9751\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.97476 to 0.97510, saving model to baseline_cnn_mitbih.h5\n",
      "Epoch 6/1000\n",
      " - 119s - loss: 0.1035 - acc: 0.9715 - val_loss: 0.0797 - val_acc: 0.9774\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.97510 to 0.97739, saving model to baseline_cnn_mitbih.h5\n",
      "Epoch 7/1000\n",
      " - 105s - loss: 0.0961 - acc: 0.9731 - val_loss: 0.0739 - val_acc: 0.9791\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.97739 to 0.97910, saving model to baseline_cnn_mitbih.h5\n",
      "Epoch 8/1000\n",
      " - 110s - loss: 0.0915 - acc: 0.9740 - val_loss: 0.0683 - val_acc: 0.9812\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.97910 to 0.98116, saving model to baseline_cnn_mitbih.h5\n",
      "Epoch 9/1000\n",
      " - 100s - loss: 0.0876 - acc: 0.9746 - val_loss: 0.0681 - val_acc: 0.9794\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.98116\n",
      "Epoch 10/1000\n",
      " - 113s - loss: 0.0830 - acc: 0.9765 - val_loss: 0.0685 - val_acc: 0.9826\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.98116 to 0.98264, saving model to baseline_cnn_mitbih.h5\n",
      "Epoch 11/1000\n",
      " - 101s - loss: 0.0793 - acc: 0.9769 - val_loss: 0.0620 - val_acc: 0.9833\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.98264 to 0.98333, saving model to baseline_cnn_mitbih.h5\n",
      "Epoch 12/1000\n",
      " - 114s - loss: 0.0758 - acc: 0.9783 - val_loss: 0.0604 - val_acc: 0.9838\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.98333 to 0.98378, saving model to baseline_cnn_mitbih.h5\n",
      "Epoch 13/1000\n",
      " - 71s - loss: 0.0733 - acc: 0.9790 - val_loss: 0.0673 - val_acc: 0.9806\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.98378\n",
      "Epoch 14/1000\n",
      " - 73s - loss: 0.0719 - acc: 0.9788 - val_loss: 0.0620 - val_acc: 0.9829\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.98378\n",
      "Epoch 15/1000\n",
      " - 75s - loss: 0.0683 - acc: 0.9799 - val_loss: 0.0619 - val_acc: 0.9831\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.98378\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 16/1000\n",
      " - 96s - loss: 0.0513 - acc: 0.9849 - val_loss: 0.0469 - val_acc: 0.9870\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.98378 to 0.98698, saving model to baseline_cnn_mitbih.h5\n",
      "Epoch 17/1000\n",
      " - 89s - loss: 0.0465 - acc: 0.9860 - val_loss: 0.0469 - val_acc: 0.9866\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.98698\n",
      "Epoch 18/1000\n",
      " - 92s - loss: 0.0431 - acc: 0.9869 - val_loss: 0.0458 - val_acc: 0.9870\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.98698\n",
      "Epoch 19/1000\n",
      " - 92s - loss: 0.0409 - acc: 0.9871 - val_loss: 0.0449 - val_acc: 0.9869\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.98698\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 20/1000\n",
      " - 76s - loss: 0.0401 - acc: 0.9877 - val_loss: 0.0451 - val_acc: 0.9862\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.98698\n",
      "Epoch 21/1000\n",
      " - 100s - loss: 0.0398 - acc: 0.9879 - val_loss: 0.0450 - val_acc: 0.9866\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.98698\n",
      "Epoch 00021: early stopping\n",
      "Test f1 score : 0.9106154865262601 \n",
      "Test accuracy score : 0.9834642791887448 \n"
     ]
    }
   ],
   "source": [
    "#get the model and save it in an h5 binary file\n",
    "model = get_model()\n",
    "file_path = \"baseline_cnn_mitbih.h5\"\n",
    "\n",
    "#checkpointing the model's weight based on the accuracy of the model\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "#set early stopping based on accuracy improving or not. It stops after 5 epochs of no accuracy improvement\n",
    "early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5, verbose=1)\n",
    "\n",
    "#reduces learning rate when a metric has stopped improving\n",
    "redonplat = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", patience=3, verbose=2)\n",
    "\n",
    "#defining the callbacks list to include the above parameters\n",
    "callbacks_list = [checkpoint, early, redonplat]\n",
    "\n",
    "#train the model\n",
    "model.fit(X, Y, epochs=1000, verbose=2, callbacks=callbacks_list, validation_split=0.1)\n",
    "model.load_weights(file_path)\n",
    "\n",
    "#test the model\n",
    "pred_test = model.predict(X_test)\n",
    "pred_test = np.argmax(pred_test, axis=-1)\n",
    "\n",
    "#get f1 score of the model & print it. The f1 score considers the precision & recall.\n",
    "f1 = f1_score(Y_test, pred_test, average=\"macro\")\n",
    "print(\"Test f1 score : %s \"% f1)\n",
    "\n",
    "#get accuracy score of the model & print it\n",
    "acc = accuracy_score(Y_test, pred_test)\n",
    "print(\"Test accuracy score : %s \"% acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #randomly selects 10 records and displays their graphs. an observation is that because there are so many normal records, it often selects them which is simple probability\n",
    "# df = pd.read_csv(\"mitbih_test.csv\", header=None)\n",
    "# print(df.shape)\n",
    "# print(Counter(df[187].values))\n",
    "\n",
    "# Y = np.array(df[187].values).astype(np.int8)\n",
    "# X = np.array(df[list(range(187))].values)\n",
    "\n",
    "# indexes = random.sample(list(range(df.shape[0])), 10)\n",
    "\n",
    "# for i in indexes:\n",
    "\n",
    "#     data = [go.Scatter(\n",
    "#               x=list(range(187)),\n",
    "#               y=X[i, :])]\n",
    "\n",
    "#     plot({\"data\": data,\n",
    "#           \"layout\": {\"title\": \"Heartbeat Class : %s \"%Y[i]}}, filename='%s.html'%i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test f1 score : 0.7243043646463921 \n",
      "Test accuracy score : 0.9250132485426603 \n"
     ]
    }
   ],
   "source": [
    "#testing the model on abnormal cases only (categories 1,2,3,4)\n",
    "df_test_ab = pd.read_csv(\"/Users/lujainmohd99/Desktop/ecg-classification/attempt2/ECG_Heartbeat_Classification/code/abs.csv\", header=None)\n",
    "\n",
    "Y_test_ab = np.array(df_test_ab[187].values).astype(np.int8)\n",
    "X_test_ab = np.array(df_test_ab[list(range(187))].values)[..., np.newaxis]\n",
    "\n",
    "#test the model\n",
    "pred_test_ab = model.predict(X_test_ab)\n",
    "pred_test_ab = np.argmax(pred_test_ab, axis=-1)\n",
    "\n",
    "#get f1 score of the model & print it. The f1 score considers the precision & recall.\n",
    "f1_ab = f1_score(Y_test_ab, pred_test_ab, average=\"macro\")\n",
    "print(\"Test f1 score : %s \"% f1_ab)\n",
    "\n",
    "#get accuracy score of the model & print it\n",
    "acc_ab = accuracy_score(Y_test_ab, pred_test_ab)\n",
    "print(\"Test accuracy score : %s \"% acc_ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
